data:
  batch_size: 32
  chr_length: 256
  stack: false
experiment:
  project_name: WaveLSTM-attentive-autoencoder
  num_epochs: 75
  run_id: clonal_${encoder.base.method}_J${encoder.waveLSTM.J}R${attention.r_hops}
  save_file: logs/${experiment.run_id}.pkl
  train: true
  verbose: true
attention:
  r_hops: 1
  attention_unit: 350
encoder:
  base:
    D: 3
    method: waveLSTM
  waveLSTM:
    wavelet: haar
    J: 4
    layers: 1
    hidden_channels: 256
    proj_size: 64
decoder:
  base:
    method: rccae
  rccae: null
  fc: null
